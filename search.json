[
  {
    "objectID": "posts/gatsby.html",
    "href": "posts/gatsby.html",
    "title": "DH 140 Final Project",
    "section": "",
    "text": "For my final project, I will be analyzing the novel The Great Gatsby by F. Scott Fitzgerald. The source for my project is The Gutenberg Project, which has provided the text of the novel.\nFitzgerald’s The Great Gatsby is my favorite novel, and I first read it in a high school English class. I was enamored by the themes of love and the American dream as well as the lively setting and time period. During my first read of the book, I analyzed the texts from a literary standpoint. Now, I hope to perform a literary analysis from a statistical point of view. Using the raw text of the novel, I ask “What are the overal moods and themes of the novel?” and “How does the sentiment of the novel reflect the historical time period that it was set in?”\n\n\n\n\n\nAs stated earlier, my dataset is the raw text of The Great Gatsby provided by Project Gutenberg. The first step in my data processing was to capture and store the text of the novel. I used the Python requests library, which allows us to use a target url and extract the text from the webpage. Project Gutenberg stores the text in a textfile format, so this was straightforward. From there, I did some simple processing like lowering the case of all of the letters and removing the Project Gutenberg text additions to the page. What remained was one long string with the text of The Great Gatsby. Using this string, I created a Python list where each element is a word from the string.\nStopwords are a list of words that are commonly filtered out before natural language processing since they are insignificant. Example stopwords are “are”, “is”, “the”, and “the”. The Python Natural Language Tool Kit provides a list of stopwords for a specified language. So, anytime I was processing the words from the Novel and testing their sentiment/frequencies, I filtered out stopwords.\nFitzgerald divided his novel into 9 chapters, and the chapters are titled as roman numerals in the text. I decided to split up the string of words into 9 separate strings corresponding to each chapter so I could compare them.\nUsing the list of all words in the novel and the 9 lists of words in each chapter, minus the stop words, I performed my data analysis. I used the Vader sentiment analyzer, the word frequencies, the positive/negative word lexicon, and the Porter Stemmer to draw various conclusions and make comparisons.\n\nimport requests\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import vader\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\nnltk.download('opinion_lexicon')\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n\n\nTrue\n\n\n\ntarget_url = \"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\"\nresponse = requests.get(target_url)\ngatsby_string = response.text\ngatsby_string = gatsby_string.split('*** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***')[1]\ngatsby_string = gatsby_string.split('*** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***')[0]\ngatsby_string = gatsby_string.lower()\n\n\nstopwords_list = stopwords.words('english')\nsia = vader.SentimentIntensityAnalyzer()\n\n\ngatsby_word_list = [w for w in word_tokenize(gatsby_string.lower())]\n\n\nreviewPolarity = 0.0\npos_list = []\nneg_list = []\nfor word in gatsby_word_list:\n  val = sia.polarity_scores(word)['compound']\n  if val > 0:\n    pos_list.append(word)\n  elif val < 0:\n    neg_list.append(word)\nprint(pos_list[0:10])\nprint(neg_list[0:10])\n\n['great', 'lover', 'lover', 'like', 'advantages', 'great', 'curious', 'matter', 'hope', 'tolerance']\n['cry', 'vulnerable', 'criticizing', 'victim', 'bores', 'accused', 'hostile', 'missing', 'forget', 'snobbishly']\n\n\n\ntop15_pos = nltk.FreqDist(pos_list).most_common(15)\ntop15_neg = nltk.FreqDist(neg_list).most_common(15)\n\nprint(top15_pos)\nprint(top15_neg)\n\n[('like', 119), ('hand', 63), ('want', 59), ('well', 53), ('good', 30), ('great', 26), ('love', 24), ('god', 20), ('matter', 19), ('loved', 19), ('friend', 18), ('party', 18), ('won', 17), ('better', 17), ('bright', 16)]\n[('no', 97), ('miss', 38), ('cried', 31), ('alone', 25), ('demanded', 24), ('broke', 23), ('stopped', 19), ('hard', 17), ('shook', 17), ('crazy', 14), ('stop', 13), ('bad', 11), ('leave', 11), ('lost', 11), ('killed', 11)]\n\n\n\nno_stopwords = [w for w in word_tokenize(gatsby_string.lower()) if w not in stopwords_list]\n\nreviewPolarity = 0.0\npos_list = []\nneg_list = []\nfor word in no_stopwords:\n  val = sia.polarity_scores(word)['compound']\n  if val > 0:\n    pos_list.append(word)\n  elif val < 0:\n    neg_list.append(word)\ntop15_pos = nltk.FreqDist(pos_list).most_common(15)\ntop15_neg = nltk.FreqDist(neg_list).most_common(15)\n\nprint(top15_pos)\nprint(top15_neg)\n\n[('like', 119), ('hand', 63), ('want', 59), ('well', 53), ('good', 30), ('great', 26), ('love', 24), ('god', 20), ('matter', 19), ('loved', 19), ('friend', 18), ('party', 18), ('better', 17), ('bright', 16), ('yes', 16)]\n[('miss', 38), ('cried', 31), ('alone', 25), ('demanded', 24), ('broke', 23), ('stopped', 19), ('hard', 17), ('shook', 17), ('crazy', 14), ('stop', 13), ('bad', 11), ('leave', 11), ('lost', 11), ('killed', 11), ('dead', 11)]\n\n\n\n\n\n\nIn the above section I determined what the 15 most frequent positively conotated and negatively conotated words in the novel. First, I separated the words in the novel by polarity scores, negative indicating a negative sentiment and positive indicating a positive sentiment. From there, I used the frequency distributor from NLTK to return the top 15 in each category. I did this twice, once with stop words and once with stop words removed. The results from the two iterations were nearly the same, with the only stop word in the first iteration of positive words being “won”, and the only stop word in the negative list being “no”. The top 15 positive and negative words are outputted above.\n\nstemmer = PorterStemmer()\nunstemmed = [w for w in word_tokenize(gatsby_string.lower()) if w not in stopwords_list]\nstemmed = []\nfor word in unstemmed:\n  stemmed.append(stemmer.stem(word))\n\nreviewPolarity = 0.0\npos_list = []\nneg_list = []\nfor word in stemmed:\n  val = sia.polarity_scores(word)['compound']\n  if val > 0:\n    pos_list.append(word)\n  elif val < 0:\n    neg_list.append(word)\ntop15_pos = nltk.FreqDist(pos_list).most_common(15)\ntop15_neg = nltk.FreqDist(neg_list).most_common(15)\n\nprint(top15_pos)\nprint(top15_neg)\n\n[('like', 128), ('hand', 103), ('want', 97), ('love', 56), ('well', 53), ('good', 30), ('great', 26), ('laugh', 25), ('matter', 24), ('play', 24), ('friend', 24), ('reach', 23), ('smile', 22), ('care', 21), ('god', 20)]\n[('miss', 42), ('stop', 35), ('demand', 28), ('broke', 23), ('hard', 17), ('shook', 17), ('drop', 14), ('kill', 12), ('bad', 11), ('lost', 11), ('dead', 11), ('forget', 10), ('low', 10), ('violent', 10), ('interrupt', 10)]\n\n\n\n\n\nIn this section above, I did a nearly identical analysis as the prior section. The only difference this time around was that I used stemmed words instead of the raw word. From their documentation, the Porter Stemmer “removed the commoner morphological and inflexional endings from words in English.” For example, the words “lover”, “lovely”, and “loved” would all boil down to the stem “love”. After this analysis, I noticed slightly different results than before. For instance in the positive word list, “love” is more frequent after being stemmed, and “smile” and “care” made it onto the list after being stemmed. In the negative word list, “stop” moved up in frequency after being stemmed.\nThe results were not wildly different after being stemmed, but it was interesting to notice the slight change.\n\nfirst_ch_string = 'In my younger and more vulnerable years'\nsecond_ch_string ='About halfway between West Egg and New York the motor road hastily'\nthird_ch_string ='There was music from my neighbour’s house through the summer nights.'\nfourth_ch_string = 'On Sunday morning while church bells'\nfifth_ch_string = 'When I came home to West Egg that night'\nsixth_ch_string = 'About this time an ambitious young reporter from New York'\nseventh_ch_string ='It was when curiosity about Gatsby was at its highest'\neighth_ch_string = 'I couldn’t sleep all night; a foghorn'\nninth_ch_string = 'After two years I remember the rest of that day'\n\nfirst_ch_index = gatsby_string.find(first_ch_string.lower())\nsecond_ch_index = gatsby_string.find(second_ch_string.lower())\nthird_ch_index = gatsby_string.find(third_ch_string.lower())\nfourth_ch_index = gatsby_string.find(fourth_ch_string.lower())\nfifth_ch_index = gatsby_string.find(fifth_ch_string.lower())\nsixth_ch_index = gatsby_string.find(sixth_ch_string.lower())\nseventh_ch_index = gatsby_string.find(seventh_ch_string.lower())\neighth_ch_index = gatsby_string.find(eighth_ch_string.lower())\nninth_ch_index = gatsby_string.find(ninth_ch_string.lower())\n\n\nscore_arr = []\n\ngatsby_sections = []\ngatsby_sections.append(gatsby_string[first_ch_index:second_ch_index])\ngatsby_sections.append(gatsby_string[second_ch_index:third_ch_index])\ngatsby_sections.append(gatsby_string[third_ch_index:fourth_ch_index])\ngatsby_sections.append(gatsby_string[fourth_ch_index:fifth_ch_index])\ngatsby_sections.append(gatsby_string[fifth_ch_index:sixth_ch_index])\ngatsby_sections.append(gatsby_string[sixth_ch_index:seventh_ch_index])\ngatsby_sections.append(gatsby_string[seventh_ch_index:eighth_ch_index])\ngatsby_sections.append(gatsby_string[eighth_ch_index:ninth_ch_index])\ngatsby_sections.append(gatsby_string[ninth_ch_index:])\n\npos_score_per_section = [0,0,0,0,0,0,0,0,0]\nneg_score_per_section = [0,0,0,0,0,0,0,0,0]\nsection_scatters = []\nfor section in range(len(gatsby_sections)):\n  section_list = [w for w in word_tokenize(gatsby_sections[section].lower()) if w not in stopwords_list]\n  section_points = []\n  score = 0\n  pos_val = 0\n  neg_val = 0\n  for word in section_list:\n    val = sia.polarity_scores(word)['compound']\n    score += val\n    if (val >= 0):\n      pos_val += val\n    else:\n      neg_val += val\n    if val != 0:\n      section_points.append(val)\n  pos_score_per_section[section] = pos_val\n  neg_score_per_section[section] = neg_val*-1\n  section_scatters.append(section_points)\n  score_arr.append(score)\n\nx_coordinate = [ 1+i for i in range(len(score_arr)) ]\nplt.title('Sentiment of the Sections of Gatsby')\nplt.bar(x_coordinate, score_arr)\n\n<BarContainer object of 9 artists>\n\n\n\n\n\n\npos_series = pd.Series(pos_score_per_section, index=[1,2,3,4,5,6,7,8,9])\nneg_series = pd.Series(neg_score_per_section, index=[1,2,3,4,5,6,7,8,9])\ndf = pd.DataFrame({\"positive\":pos_score_per_section,\"negative\":neg_score_per_section})\nax = df.plot.bar(color=[\"Green\",\"Red\"], rot=0, title=\"Balance of Positve and Negative Words in Each Section\")\nax.set_xlabel(\"chapter\")\nax.set_ylabel(\"score\")\nax.xaxis.set_major_formatter(plt.FixedFormatter([1,2,3,4,5,6,7,8,9]))\nplt.show()\n\n\n\n\n\n\n\nAbove, I calculated the positivity score and negativity score of each chapter of the novel. The first graph with single bars uses a single score for each section. Positive and negative sentiment scores for each word in the section are added to a sum and plotted. In the second graph with double bars, I kept separate counts for the positive score and negative score for each chapter. Both are important representations, but I think that the double bar graph allows us to visualize the balance of positive to negative words in each chapter.\n\npositive_words = set(opinion_lexicon.positive())\nnegative_words = set(opinion_lexicon.negative())\n\n\nscore_arr = []\npos_score_per_section = [0,0,0,0,0,0,0,0,0]\nneg_score_per_section = [0,0,0,0,0,0,0,0,0]\nfor section in range(len(gatsby_sections)):\n  section_list = [w for w in word_tokenize(gatsby_sections[section].lower()) if w not in stopwords_list]\n  score = 0\n  pos_val = 0\n  neg_val = 0\n  for word in section_list:\n    if word in positive_words:\n      score += 1\n      pos_val += 1\n    elif word in negative_words:\n      score -= 1\n      neg_val += 1\n  pos_score_per_section[section] = pos_val\n  neg_score_per_section[section] = neg_val\n  score_arr.append(score)\n\nx_coordinate = [ 1+i for i in range(len(score_arr)) ]\nplt.title('Alternate Plot of Section Sentiment') \nplt.bar(x_coordinate, score_arr)\n\n<BarContainer object of 9 artists>\n\n\n\n\n\n\npos_series = pd.Series(pos_score_per_section, index=[1,2,3,4,5,6,7,8,9])\nneg_series = pd.Series(neg_score_per_section, index=[1,2,3,4,5,6,7,8,9])\ndf = pd.DataFrame({\"positive\":pos_score_per_section,\"negative\":neg_score_per_section})\nax = df.plot.bar(color=[\"Green\",\"Red\"], rot=0, title=\"Alternate Balance of Positve and Negative Words in Each Section\")\nax.set_xlabel(\"chapter\")\nax.set_ylabel(\"score\")\nax.xaxis.set_major_formatter(plt.FixedFormatter([1,2,3,4,5,6,7,8,9]))\nplt.show()\n\n\n\n\n\n\n\nSimilar to the prior section, I plotted the sentiment scores for each section, but I calculated the scores differently. This time, instead of summing up the raw sentiment score, which is a decimal, I added one for a positive score (anything above zero) and subtracted 1 for a negative score (anything below zero). This normalizes the sentiment for each word, and allows us to view the balance of positive and negative words in regards to their frequencies as opposed to how positive or negative they are. The results are fairly different than the prior section, which is an interesting juxtaposition.\n\nx = [w for w in word_tokenize(gatsby_string.lower()) if w not in stopwords_list and w.isalnum()]\ndf = pd.DataFrame({'words': nltk.FreqDist(x).keys(),\n                   'frequencies': nltk.FreqDist(x).values()})    \n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      words\n      frequencies\n    \n  \n  \n    \n      0\n      great\n      26\n    \n    \n      1\n      gatsby\n      251\n    \n    \n      2\n      scott\n      1\n    \n    \n      3\n      fitzgerald\n      1\n    \n    \n      4\n      table\n      31\n    \n  \n\n\n\n\n\nprint(df.describe())\n\n       frequencies\ncount  5483.000000\nmean      4.053073\nstd      10.400495\nmin       1.000000\n25%       1.000000\n50%       1.000000\n75%       3.000000\nmax     251.000000\n\n\n\ntop_10_words = df.nlargest(10, 'frequencies')\nprint(top_10_words)\n\n      words  frequencies\n1    gatsby          251\n298    said          233\n547     tom          188\n549   daisy          183\n184     one          145\n40     like          119\n79     came          108\n147    back          107\n118  little          103\n164     man           98\n\n\n\ntop10_pos_words = []\ntop10_neg_words = []\nfor i in range(10):\n    top10_pos_words.append(top15_pos[i][0])\n    top10_neg_words.append(top15_neg[i][0])\n    \npos_df = df.loc[df['words'].isin(top10_pos_words)]\npos_df.head()\n\n\n\n\n\n  \n    \n      \n      words\n      frequencies\n    \n  \n  \n    \n      0\n      great\n      26\n    \n    \n      40\n      like\n      119\n    \n    \n      114\n      matter\n      19\n    \n    \n      713\n      hand\n      63\n    \n    \n      816\n      laugh\n      9\n    \n  \n\n\n\n\n\nneg_df = df.loc[df['words'].isin(top10_neg_words)]\nneg_df.head()\n\n\n\n\n\n  \n    \n      \n      words\n      frequencies\n    \n  \n  \n    \n      140\n      hard\n      17\n    \n    \n      831\n      miss\n      38\n    \n    \n      939\n      drop\n      4\n    \n    \n      1055\n      broke\n      23\n    \n    \n      1121\n      bad\n      11\n    \n  \n\n\n\n\n\npos_df.plot.bar(x='words', y='frequencies', rot=0, title='Frequencies of Top 10 Positive Words')\n\n<AxesSubplot: title={'center': 'Frequencies of Top 10 Positive Words'}, xlabel='words'>\n\n\n\n\n\n\nneg_df.plot.bar(x='words', y='frequencies', rot=0, title='Frequencies of Top 10 Negative Words')\n\n<AxesSubplot: title={'center': 'Frequencies of Top 10 Negative Words'}, xlabel='words'>\n\n\n\n\n\n\n\n\nIn this section, I looked into the word frequencies in the novel. There are over 5000 unique words in the novel, with the average frequency being around 4 times. I looked into the frequencies of names of main characters in the novel. Gatsby is the name that appears most often, followed by Tom, Daisy, and Nick. Originally, I was planning on examining sentences that these names appeared in to analyze the overal sentiment of that sentence. I thought this could tell us about the reputation and theme of the character. I decided this might be difficult though, since a sentence mentioning a name may not be representative of reputation/theme. For instance, a lot of the name occurrences may be in phrases like “Gatsby said” or other verb phrases, which wouldn’t reveal anything insightful.\nI determined what the most frequent 10 words in the novel are: gatsby, said, tom, daisy, one, like, came, back, little, man. I also looked into the split of positive and negative words in the novel and found the top 10 frequent words in those lists. The most frequent positive words are: great, like, matter, hand, laugh, love, well, want, good, play. The most frequent negative words are: hard, miss, drop, broke, bad, shook, stop, demand, lost, kill.\n\n\n\n\nBased on the word frequencies and sentiment visualizations, we can draw conclusions about the themes/mood of the novel and the overall sentiment arc of the story based on its chapters. This is also reflective of the setting of the novel: the Roaring 20s in New York City. If we look at the most frequent words in the novel, there are three names: Gatsby, Daisy, and Tom. The novel famously contains a love triangle between these three characters, and the data aligns. Other than those names, some notable frequent words were “back”, “man”, and “one”. Although these words are definitely part of phrases that have significance, it is interesting that those single words appeared so much. The Great Gatsby is a story about Jay Gatsby, a man that came into money and had a haunting past. A common theme in the book is his longing for another character, Daisy. The word “one” frequently appearing could allude to the idea that Gatsby was always obsessed with the idea of Daisy being his “one” and him coming into so much money and ambition with the singular goal of catching just Daisy’s eye. The word “back” could allude to the themes of the past, present, and future that Fitzgerald wrote about in his novel. The book famously ends with the quote “So we beat on, boats against the current, borne back ceaselessly into the past.” The words “past” and “back” may refer to each other.\nAnother interesting conclusion comes from the breakdown of the 10 most frequent positive words and 10 most frequent negative words in the book. Amongst the positive words are “great”, “love”, and “play” which I think are three words that perfectly describe the life of the wealthy during the Roaring 20s in America. Especially in the novel, wealthy characters like Gatsby and Daisy host and attend “great” lavish parties, “play” music, sports, and games, and spend their time chasing “love”. Amongst the negative words are “broke”, “kill”, and “miss”. During the 20s, the wealth gap was massive. Many families lived paycheck to paycheck and in worse parts of town, alluding to the word “broke”. In Gatsby, a character dies and another character is famously framed, hence the presence of “kill”. Lastly, the word “miss” refers to the theme of longing and absence Gatsby often felt about Daisy.\nThe last visualizations I would like to discuss are the sentiment charts separated by chapters in the book. Chapter 7 has the most number of positive and negative words, with the balance being skewed very negative when looking at the normalized scores. This is the climax of the book and where (spoiler alert!) Myrtle is killed, so it makese sense that the sentiment is heavily negative. Chapter 1 is the only chapter where the normalized positive word score is higher than the negative score. I conclude that this is true because we are introduced to Gatsby and his lavish parties in this chapter, and so far, we have a positive view of his life. It is also interesting because in Chapter 7, the group drives through a rough part of town with folks of lower social classes, so its sentiment is far more negative to the pretty houses and lavish parties in Chapter 1 – a nicer portrayal of the 20s.\nOverall, this analysis aligned with my literary analysis from reading The Great Gatsby. It is interesting because when analyzing literary work, often one needs to read between the lines. However, when looking at the lines as raw data, we can draw similar conclusions. The Great Gatsby is a story about love, death, ambition, greatness, and the past/present/future, both between the lines and through the lines themselves."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "DH 140 Final Project\n\n\n\n\n\nAn Analysis on The Great Gatsby\n\n\n\n\n\n\nMar 19, 2023\n\n\nMuthu Palaniappan\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]